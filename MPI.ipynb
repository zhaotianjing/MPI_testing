{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to MPI\n",
    "-----\n",
    "Tianjing Zhao 09/09/2019\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal for today: \n",
    "* how dose MPI work? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is MPI?\n",
    "\n",
    "* Message Passing Interface (MPI)\n",
    "* passing message between different process(rank)\n",
    "* all process(rank) will run code at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Point-to-Point Communication\n",
    "\n",
    "### Definition\n",
    "\n",
    "pass data from one process to another\n",
    "\n",
    "\n",
    "### Functions\n",
    "\n",
    "* Send(obj::T, dest::Integer, tag::Integer, comm::Comm) \n",
    "\n",
    "* Recv!(buf::Array{T}, src::Integer, tag::Integer, comm::Comm)\n",
    "\n",
    "### Example\n",
    "\n",
    "[send_receive.jl](https://github.com/zhaotianjing/MPI_testing/blob/master/send_receive.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collective Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Broadcasting\n",
    "###  Definition\n",
    "Broadcasting sends a message to all processes(rank)\n",
    "###  Function\n",
    "* bcast(obj, root::Integer, comm::MPI.Comm) \n",
    "\n",
    "###  Example\n",
    "[broadcast2.jl](https://github.com/zhaotianjing/MPI_testing/blob/master/broadcast2.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Scattering\n",
    "###  Definition\n",
    "Scatter takes an array and distributes contiguous sections of it across all ranks (from root rank to all rank)\n",
    "\n",
    "![](broadcast.png)\n",
    "\n",
    "[image1](https://github.com/zhaotianjing/MPI_testing/blob/master/broadcast.png)\n",
    "\n",
    "###  Function\n",
    "* Scatter(sendbuf, count, root, comm)\n",
    "\n",
    "### Example\n",
    "[scatter.jl](https://github.com/zhaotianjing/MPI_testing/blob/master/scatter.jl)\n",
    "\n",
    "\n",
    "## 2.2.1 User-defined scatter\n",
    "###  Definition\n",
    "Scatter message of length count[j] to rank[j]\n",
    "\n",
    "<img src=\"scatterv.png\" width=\"300\">\n",
    "\n",
    "[image2](https://github.com/zhaotianjing/MPI_testing/blob/master/scatterv.png)\n",
    "\n",
    "### Function\n",
    "* Scatterv(sendbuf, counts, root, comm)\n",
    "\n",
    "\n",
    "\n",
    "### Example\n",
    "[scatterv.jl](https://github.com/zhaotianjing/MPI_testing/blob/master/scatterv.jl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Gathering\n",
    "### Definition\n",
    "\n",
    "The reverse of scattering. Each process sends and gather message to the root process.\n",
    "\n",
    "![](gather.png)\n",
    "\n",
    "[image2](https://github.com/zhaotianjing/MPI_testing/blob/master/gather.png)\n",
    "\n",
    "### Function\n",
    "* Gather(sendbuf, count, root, comm)\n",
    "\n",
    "### Example\n",
    "[gather.jl](https://github.com/zhaotianjing/MPI_testing/blob/master/gather.jl)\n",
    "\n",
    "## 2.3.1 User-defined gather\n",
    "###  Definition\n",
    "Gather message of length count[j] from rank[j] to root\n",
    "\n",
    "<img src=\"gatherv.png\" width=\"300\">\n",
    "\n",
    "[image2](https://github.com/zhaotianjing/MPI_testing/blob/master/gatherv.png)\n",
    "\n",
    "### Function\n",
    "* Gatherv(sendbuf, counts, root, comm)\n",
    "\n",
    "\n",
    "\n",
    "### Example\n",
    "[gatherv.jl](https://github.com/zhaotianjing/MPI_testing/blob/master/gatherv.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Reduction\n",
    "### Definition\n",
    "\n",
    "Advanced gathering (The MPI reduce operation takes values in from an array on each process and reduces them to a single result on the root process.)\n",
    "![](mpi_reduce_1.png)\n",
    "![](mpi_reduce_2.png)\n",
    "\n",
    "[image3](https://github.com/zhaotianjing/MPI_testing/blob/master/mpi_reduce_1.png)\n",
    "[image4](https://github.com/zhaotianjing/MPI_testing/blob/master/mpi_reduce_2.png)\n",
    "\n",
    "### Function\n",
    "* Reduce(sendbuf, operation, root, comm)\n",
    "\n",
    "### Example - dot product using MPI\n",
    "[reduction.jl](https://github.com/zhaotianjing/MPI_testing/blob/master/reduction.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Use MPI on farm server\n",
    "\n",
    "## Submit script\n",
    "* #SBATCH --nodes=2  number of computers required\n",
    "* #SBATCH --ntasks=10 number of ranks in total\n",
    "\n",
    "## Example\n",
    "[submit_MPI.sh](https://github.com/zhaotianjing/MPI_testing/blob/master/submit_MPI.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Why MPI is used for parallel computing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Can my code be parallelize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* time consuming: data communication\n",
    "* Julia's original well-designed method is usually faster than MPI (LinearAlgebra.dot() is faster than above MPI dot product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* [MPI.jl](https://github.com/JuliaParallel/MPI.jl)\n",
    "* [parallel-programming-with-mpi-for-python](https://rabernat.github.io/research_computing/parallel-programming-with-mpi-for-python.html)\n",
    "* [Self-Consistent MPI Performance Guidelines](https://ieeexplore.ieee.org/ielx5/71/5439161/05184825.pdf?tp=&arnumber=5184825&isnumber=5439161&ref=)\n",
    "* [mpi tutorial](https://mpitutorial.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
